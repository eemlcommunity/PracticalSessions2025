# [[EEML2025](https://www.eeml.eu)] Tutorial 1: Training Vision Transformer (ViT) encoders from scratch

**Authors:** Liliane Momeni and Nikhil Parthasarathy


--- 

This tutorial is designed to provide a hands-on understanding of how to train powerful Vision Transformer (ViT) encoders from scratch ðŸš€. Part 1 will recap why ViT encoders are important, before diving into the three fundamental training paradigms. Part 2 will cover the classic fully supervised approach using labeled data. Part 3 will explore the self-supervised paradigm through Masked Autoencoders, where models learn rich features from the data itself without labels. Finally, Part 4 covers contrastive learning with noisy supervision, using CLIP as our case study.

### Outline:

- Part 1: Introduction to Vision Transformer encoders
- Part 2: Fully supervised ViT for Image Classification
- Part 3: Self-supervised Learning with Masked Autoencoders (MAE)
- Part 4 (Take-home task): Contrastive Language-Image Pre-training (CLIP)
- Part 5: Conclusion


### Notebooks

Tutorial: [![Open In 
Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eemlcommunity/PracticalSessions2025/blob/main/1_vision/vision_tutorial.ipynb)

Tutorial with solutions: [![Open In 
Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eemlcommunity/PracticalSessions2025/blob/main/1_vision/vision_tutorial_solutions.ipynb)

---
